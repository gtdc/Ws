# W

*think using a priority checklist of mental models*  

#### table of contents
- **[history of this repository](#history-of-this-repository)**
- **[y & goals](#y--goals)**

## history of this repository

`!` = not

In 2011 `Quinn` watched [Jeff Hawkin's talk](https://www.ted.com/talks/jeff_hawkins_on_how_brain_science_will_change_computing) on artificial general intelligence(`AGI`) & `Quinn 1` bcame obsessed with building `AGI` 2 help humanity. `Quinn 2` taught `Quinn 1` how 2 code & built an object-oriented(OO) Java implementation of [Numenta's cortical learning algorithm version 2(`CLA v2`)](https://github.com/WalnutiQ/wAlnut/tree/MARK_II). After running unsuccessful vision experiments using the `CLA v2` `Quinn 3` began 2 think there must b 1+ better options 2 build `AGI`. Hunter Nathanael Smith died by suicide & `Quinn 3` bcame `Q`. `Q 1` found 1 better approach in Dileep George's [PhD thesis](https://github.com/WalnutiQ/papers/blob/master/Dileep_George_PGM/HowTheBrainMightWork.pdf) & this project changed in2 an [OO Python implementation of Dileep's PhD thesis](https://github.com/WalnutiQ/wAlnut/tree/MARK_III). While researching other approaches 2 `AGI` `Q 2` found [Elon Musk's ideas on AGI](https://youtu.be/h0962biiZa4)
& read [Superintelligence by Nick Bostrom](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/1501227742) with `Q 3`'s notes [here](https://github.com/WalnutiQ/wAlnut/issues/345) & realized `Quinn` & `Q` made false assumptions about building `AGI` 2 help humanity in the 1st place. The goal changed 2 researching how 2 increase human 
intelligence faster than code based intelligence. The hypothesis `Q 4` is stuck on is since AGI will `!`b limited by slow biological processes, it isn't possible 4 human intelligence 2 increase faster than code based intelligence in the long-term. Now `Q` & team r using a checklist of 129 mental models 2 brainstorm alternative solutions 2 the AGI problem. If ur interested in thinking about this 2gether e-mail ur CV 2 `emn1over12@gmail.com` :)

~ `Q 369.9/24.10/16.32/20.7/18.0/0.8/12.60.29.4 Liu`

## Y & Goals
Between 2011 to 2016 `Q` was so focused on how 2 build AGI it was so easy 4 `Q` 2 have confirmation bias 2wards only potentially positive effects of building AGI while `Q` avoided the question:
  
`Q0: How do u control something that is smarter than all of humanity combined?`

The answer is u can't. `Q` now believes AGI should `!`b built privately || publicly & instead 1 possible solution is 2 increase human intelligence with a privately built neural lace like [Neuralink](https://neuralink.com/) which u can read about [here](http://waitbutwhy.com/2017/04/neuralink.html). 

2day, many groups of imaginative, couragous, funny, nice, smart, & hard working people r trying 2 build AGI 2 help humanity. However, `Q` believes AGI shouldn't b built bcause:

1. Humans r the dominant species on Earth bcause of our intelligence.
2. If we make another species AGI smarter than us then there is no way 4 us 2 control it 
   bcause u can't control something that is more intelligent than all of humanity combined.
3. There is a very high chance that the 1st AGI will use humans 4 purposes we do `!`want. Power tends 2 corrupt & absolute   
   power corrupts absolutely. Just look @ how we treat species that r less intelligent than us.
4. We can't stop the groups of imaginative, couragous, funny, nice, smart, & hard working people that r researching AGI so now the question is:

`Q1: How do we solve the AGI control problem?`  
Using a checklist of 129 mental models `Q` has brainstormed the following possible answers:

1. Increase human intelligence faster than AGI. 
   - `Problem:` AGI's growth will b expontenial & won't b limited by slow biological processes so `Q` doesn't think this is 
     possible in the long-term. 
2. Give every1 who wants 1 a neural lace. 
   - `Problem:` `!`sure if this is healthy 4 any human as it might cause insanity since we don't fully understand the brain 
     yet. 
3. AGI development regulation.
   - `Problem:` `Q` doesn't think this is scalable since it will `!`b possible 2 monitor every AGI developer like it is 
     possible 2 monitor every nuclear bomb developer. & as knowledge of how 2 build AGI increases more people with self-
     serving motives can create an AGI thinking they can control it & without understanding the consequences.
4. Figure out how 2 backwards time + universe travel ethically.
   - `Problem:` `Q` only knows 1 person named `LEX` that will take `Q` seriously so far :) 
5. Something `Q` hasn't || can't even imagine yet AKA a black swan event. 

Using Occam's razor `2` is the possible answer with the least number of assumptions so now the questions becomes `Q2` & `Q3`:
  
`Q2: How do u create a safe neural lace 4 any1 who wants 1?`.

1. Use it 2 help mentally disabled 1st.
2. `Q` isn't sure giving a neural lace 2 a mentally "healthy" person is healthy as our lack of full understanding of the brain may cause unexpected insanity in the wearer.
3. Privately research the best cyber security 2 b used 4 people with neural lace. 

`Q3: How do u fully understand the human brain without building 1 improved version of it in code?`

1. Non-Answer: In all other cases 2 truly understand a thing just rebuild 1 better version of it. However, in this case that means building an AGI. 

`Q` thinks the focus should b on `Q4`:

`Q4: How do u help mentally disabled people with a neural lace?`

1. Use it 2 treat 1 mental illness @ a time. Solving 4 the simplest mental illness 2 treat b4 solving more complex mental illnesses.

`Q5: What would b the simplest mental illness 2 treat with neural lace?`

~ `Q 285`
